{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6ce9a-e975-44a1-8675-e4ec03c97175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import requests\n",
    "from os import path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from rectangle import Rectangle\n",
    "import helpers\n",
    "from helpers import *\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "kindle_ar = (1430,1080)\n",
    "\n",
    "if not path.exists(f'dump'):\n",
    "    os.mkdir('dump')\n",
    "\n",
    "manga_name = 'Akira'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be491f67-9ed8-4e3f-80f4-34f40643abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += \":\"+os.getcwd()\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "\n",
    "# Set up Firefox options\n",
    "firefox_options = Options()s\n",
    "firefox_options.add_argument('--headless')  # Run Firefox in headless mode\n",
    "firefox_options.binary_location = '/usr/bin/firefox'  # Path to Firefox binary\n",
    "\n",
    "# Set up the Firefox WebDriver\n",
    "service = Service('./geckodriver')\n",
    "driver = webdriver.Firefox(service=service, options=firefox_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6e681-12d9-4305-920d-7617c219d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://mangasee123.com/manga/\"+manga_name)\n",
    "# with open('mangasee.html', 'w', encoding='utf-8') as f:\n",
    "#     f.write(driver.page_source)\n",
    "# selector = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'div.ShowAllChapters')))\n",
    "# selector.click()\n",
    "\n",
    "chapter_links = [a.get_attribute('href') for a in driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"read-online\"]')]\n",
    "\n",
    "chapter_links = reversed(chapter_links)\n",
    "pages = {}\n",
    "for chapter in chapter_links:\n",
    "    driver.get(chapter)\n",
    "\n",
    "    selector = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'i.fa-arrows-alt-v')))\n",
    "    selector.click()\n",
    "\n",
    "    img_links = [img.get_attribute('src') for img in driver.find_elements(By.CSS_SELECTOR, 'img.img-fluid')]\n",
    "    pages[chapter.split('/')[-1]] = img_links\n",
    "with open(manga_name+'.pkl', 'wb') as f:\n",
    "    pickle.dump(pages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cef4a2-5c1c-4513-9105-32ea5537093d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_page(url, draw_panel=False):\n",
    "    try:\n",
    "        raw_img = requests.get(url, stream=True).raw\n",
    "        img = np.array(Image.open(raw_img))\n",
    "        if img.ndim==2:\n",
    "            img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "    # if not path.exists(f'dump/{manga_name}/{chapter_counter:04d}'):\n",
    "    #     os.mkdir(f'dump/{manga_name}/{chapter_counter:04d}')\n",
    "    # img = cv2.imread('0012-002.png')\n",
    "    # plot_crops(img, [])\n",
    "    contours = get_contours(img)\n",
    "    contours = remove_specks_and_lines(img, contours)\n",
    "    crops = get_crops(img, contours)\n",
    "    crops = merge_overlapping_crops(crops)\n",
    "    crops = remove_small_crops(img, crops)\n",
    "    helpers.debug = False\n",
    "\n",
    "    text_crops = get_text_crops(img)\n",
    "    text_crops = remove_small_crops(img, text_crops, factor=0.5)\n",
    "\n",
    "    stats = compute_statistics(img, crops)\n",
    "    crops.extend(text_crops)\n",
    "    crops = merge_overlapping_crops(crops)\n",
    "\n",
    "    # debug = True\n",
    "    # plot_frames(img, crops, True)\n",
    "    edge_graph = build_crops_edge_graph(crops, img.shape)\n",
    "    # print(*[(i,j) for i,j in enumerate(edge_graph)], sep='\\n')\n",
    "    sorted_crops = sort_edge_graph(crops, edge_graph)\n",
    "    idx = 0\n",
    "    if not sorted_crops:\n",
    "        sorted_crops = [Rectangle(0,0,img.shape[1],img.shape[0])]\n",
    "    if stats['ignored']>0.20 and stats['overlapped']>0.01: \n",
    "        sorted_crops = [Rectangle(0,0,img.shape[1],img.shape[0])]\n",
    "    if len(sorted_crops)==1:\n",
    "        sorted_crops = [Rectangle(0,0,img.shape[1],img.shape[0])]\n",
    "\n",
    "    image = plot_frames(img, sorted_crops, True, show=helpers.debug)\n",
    "\n",
    "    to_write = []\n",
    "    for c in sorted_crops:\n",
    "        gray_crop = cv2.cvtColor(img[c.y:c.z, c.x:c.w], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if gray_crop.shape[1]/gray_crop.shape[0]>0.80 or c.area < img.shape[0]*img.shape[1]*0.5:\n",
    "            to_write.append(gray_crop)\n",
    "        elif len(sorted_crops)==1:\n",
    "            text_crops = merge_vertical_overlapping_crops(text_crops)\n",
    "            if len(text_crops)==0:\n",
    "                middleh = c.height//2+c.y\n",
    "            elif len(text_crops)==1:\n",
    "                middleh = c.height//2+c.y\n",
    "                if text_crops[0].y<=middleh and middleh<=text_crops[0].z:\n",
    "                    middleh = text_crops[0].y if text_crops[0].y-middleh < middleh-text_crops[0].z else text_crops[0].z\n",
    "            else: # find the widest gap\n",
    "                def pairwise(iterable):\n",
    "                    a = iter(iterable)\n",
    "                    return zip(a, a)\n",
    "                widest_gap = max([(idx,j.y-i.z) for idx,(i,j) in enumerate(list(pairwise(text_crops)))], key=lambda t: t[1])\n",
    "                middleh = text_crops[widest_gap[0]].z+widest_gap[1]//2\n",
    "            cv2.line(image, (c.x, middleh), (c.w, middleh), (0, 255, 0), thickness=3)\n",
    "            to_write.extend([h for h in np.split(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), [middleh])])\n",
    "        else:\n",
    "            to_write.append(gray_crop)\n",
    "\n",
    "    if draw_panel:\n",
    "        return image, stats\n",
    "\n",
    "    if helpers.debug:\n",
    "        plot_crops(image,[])\n",
    "    return [resizeAndPad(np.rot90(gc), kindle_ar, 255) for gc in to_write]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3af6b-d123-4792-a99f-065fa9542955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG CELL\n",
    "if not path.exists(f'dump/{manga_name}'):\n",
    "    os.mkdir(f'dump/{manga_name}')\n",
    "\n",
    "    pages = pickle.load(f)\n",
    "    \n",
    "helpers.debug = False\n",
    "\n",
    "image,stats = process_page('https://official-ongoing-2.gamindustri.us/manga/Onepunch-Man/0037-016.png', draw_panel=False)\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091be711-ae73-46b6-89b0-d56874481f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DEBUG CELL\n",
    "with open(manga_name+'.pkl', 'rb') as f:\n",
    "    pages = pickle.load(f)\n",
    "# print(pages[0])\n",
    "for urls in pages.values():\n",
    "    for url in urls:\n",
    "        image,stats = process_page(url, draw_panel=True)\n",
    "        plt.figure(figsize=(16,16))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77314c03-5b05-49cf-afed-198ad8775019",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "if not path.exists(f'dump/{manga_name}'):\n",
    "    os.mkdir(f'dump/{manga_name}')\n",
    "\n",
    "with open(manga_name+'.pkl', 'rb') as f:\n",
    "    pages = pickle.load(f)\n",
    "\n",
    "def update_result(result):\n",
    "    image, stats = result\n",
    "\n",
    "def write_result(pages, chapter_counter, page_counter):\n",
    "    idx = 0\n",
    "    for page in pages:\n",
    "        cv2.imwrite(f\"dump/{manga_name}/{chapter_counter:04d}/{page_counter:03d}-{idx:03d}.jpg\", page, [int(cv2.IMWRITE_JPEG_QUALITY), 20])\n",
    "        idx+=1\n",
    "        \n",
    "\n",
    "def call_and_save(args):\n",
    "    url, chapter_counter, page_counter = args\n",
    "    try:\n",
    "        write_result(process_page(url), chapter_counter, page_counter)\n",
    "    except Exception as e:\n",
    "        print(url, chapter_counter, page_counter, e)\n",
    "\n",
    "def chap_generator():\n",
    "    for chapter_counter,chap in enumerate([*pages.keys()]):\n",
    "        # if chapter_counter!=3: continue\n",
    "        os.system(f'mkdir -p dump/{manga_name}/{chapter_counter:04d}')\n",
    "        page0 = np.zeros([*reversed(kindle_ar)])\n",
    "        cv2.putText(img=page0, text=f'Chapter {chapter_counter:4d}', org=(150, 250), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=3, color=(255),thickness=3)\n",
    "        cv2.putText(img=page0, text=f'{chap}', org=(150, 350), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255),thickness=3)\n",
    "        page0 = np.rot90(page0)\n",
    "        page_counter = 0\n",
    "        write_result([page0], chapter_counter, page_counter)\n",
    "        for url in pages[chap]:\n",
    "            page_counter+=1\n",
    "            yield url, chapter_counter, page_counter\n",
    "\n",
    "pool = multiprocessing.Pool(64)\n",
    "for i in tqdm(pool.imap_unordered(call_and_save, chap_generator()), total=sum([len(i) for i in pages.values()])): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65054d1-26a6-454b-8c4f-099c4c167a32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find /nfs/iir/home/lpuglia/repository/kumiko -name .ipynb_checkpoints -exec rm -rf {} \\;\n",
    "# export PATH=$PATH:/nfs/iir/home/lpuglia/repository/kumiko/dump\n",
    "# /local_storage0/lpuglia/anaconda3/envs/kumiko/bin/kcc-c2e -p K34 dump/{manga_name}\n",
    "\n",
    "# os.system(f\"cd dump; zip -FSr {manga_name} {manga_name} -x '*/\\.*'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kumiko",
   "language": "python",
   "name": "kumiko"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "68a41b7eb7f97a56dfe91caaf3fc1bc75b1839613c13e501145be387320902b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
